{
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "# Run Experiments --- Non-Parametric Representation Learning with Kernels"
      ],
      "metadata": {
        "id": "MfNl4Sy66PDl"
      }
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "x_3bTztgzwbp"
      },
      "source": [
        "# Imports"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XqQjS7bgztcQ",
        "outputId": "27d139cb-a6c8-4ba3-afa1-821e1c55c83d"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<ipython-input-1-d3d044349c9b>:32: DeprecationWarning: Please use `eigsh` from the `scipy.sparse.linalg` namespace, the `scipy.sparse.linalg.eigen` namespace is deprecated.\n",
            "  from scipy.sparse.linalg.eigen import eigsh as largest_eigsh\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Mounted at /content/gdrive\n",
            "/content/gdrive/MyDrive/Kernel_Representation_Shared\n"
          ]
        }
      ],
      "source": [
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from itertools import cycle, islice\n",
        "\n",
        "\n",
        "# import sklearn\n",
        "from sklearn.metrics import pairwise_distances\n",
        "from sklearn.metrics.pairwise import pairwise_kernels\n",
        "from sklearn import cluster, datasets\n",
        "from sklearn.preprocessing import StandardScaler\n",
        "from sklearn import svm\n",
        "from sklearn.metrics.cluster import adjusted_rand_score\n",
        "\n",
        "from sklearn.model_selection import train_test_split\n",
        "from sklearn.neighbors import KNeighborsClassifier\n",
        "from sklearn.metrics import accuracy_score\n",
        "from sklearn.decomposition import KernelPCA\n",
        "from sklearn.cluster import KMeans\n",
        "import sklearn\n",
        "from sklearn import neighbors\n",
        "from sklearn.inspection import DecisionBoundaryDisplay\n",
        "from sklearn.utils import shuffle\n",
        "from sklearn import manifold\n",
        "import pandas as pd\n",
        "from sklearn.preprocessing import MinMaxScaler\n",
        "from sklearn.model_selection import LeaveOneOut\n",
        "from matplotlib.lines import Line2D\n",
        "\n",
        "# import scipy\n",
        "from scipy.linalg import eigh as largest_eigh\n",
        "from scipy.linalg import sqrtm as sqrtm\n",
        "from scipy.sparse.linalg.eigen import eigsh as largest_eigsh\n",
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/gdrive')\n",
        "\n",
        "%cd /content/gdrive/MyDrive/Kernel_Representation_Shared/"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "\n",
        "device = 'cuda:0' if torch.cuda.is_available() else 'cpu'\n",
        "device"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 36
        },
        "id": "KE4gBkgSzr5b",
        "outputId": "2eb2c614-ee6a-4dcf-a0e4-4a441abdee91"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'cpu'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 2
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MCyc0Cs1z5p3"
      },
      "source": [
        "# Helper Functions"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "##### Helper functions for Kernels\n",
        "\n",
        "def kernel_function(kernel_name, X, Y,gamma):\n",
        "    ### Helper function that we can use to call kernels in one line\n",
        "    if kernel_name == 'Gaussian':\n",
        "        k = sklearn.metrics.pairwise.rbf_kernel(X,Y,gamma = gamma)\n",
        "    elif kernel_name == 'Laplacian':\n",
        "        k = sklearn.metrics.pairwise.laplacian_kernel(X,Y,gamma = gamma)\n",
        "    elif kernel_name == 'Sigmoid':\n",
        "        k = sklearn.metrics.pairwise.sigmoid_kernel(X,Y,gamma = gamma)\n",
        "    elif kernel_name == 'Linear':\n",
        "        k = sklearn.metrics.pairwise.linear_kernel(X,Y)\n",
        "    elif kernel_name == 'ReLU':\n",
        "        k = ReLU_Kernel(X,Y,gamma)\n",
        "    elif kernel_name == 'expchi2':\n",
        "        k = sklearn.metrics.pairwise.chi2_kernel(X,Y,gamma = gamma)\n",
        "    else:\n",
        "        print('Kernel name not defined')\n",
        "\n",
        "    return k\n",
        "\n",
        "##### Define Kernels\n",
        "\n",
        "def laplacian_kernel_manual(x,y):\n",
        "    return np.exp(-gamma*np.linalg.norm(x-y,1))\n",
        "\n",
        "\n",
        "##### Define one hidden layer ReLU Kernel - See supplementary material or Bietti and Bach 2021\n",
        "\n",
        "def kappa_0(u):\n",
        "    return 1/np.pi * (np.pi - np.arccos(u))\n",
        "\n",
        "def kappa_1(u):\n",
        "    return 1/np.pi * (u*(np.pi - np.arccos(u)) + np.sqrt(1-u**2))\n",
        "\n",
        "def ReLU_Kernel(x,y,gamma):\n",
        "    inp = (x@y.T)/gamma # Remark: gamma here is added as a normalizer as we have norm one constraint on the inputs of the relu kernel\n",
        "    np.clip(inp, -1, 1, out=inp)\n",
        "\n",
        "    K_ij = inp*kappa_0(inp) + kappa_1(inp)\n",
        "    return K_ij\n",
        "\n",
        "# ReLU Kernel inputs have to have input norm one. use this to normalise if necessary\n",
        "def ReLU_Kernel_normalize(X):\n",
        "    n_unlabelled = X.shape[0]\n",
        "    max_list = []\n",
        "    for i in range(n_unlabelled):\n",
        "        for j in range(n_unlabelled):\n",
        "            max_list.append(X[i,:].reshape(1, -1)@X[j,:].reshape(1, -1).T)\n",
        "\n",
        "    gamma = np.max(max_list)\n",
        "    return gamma\n",
        "\n",
        "def ReLU_Kernel_normalize_2(X,Y):\n",
        "    n_unlabelled = X.shape[0]\n",
        "    max_list = []\n",
        "    for i in range(n_unlabelled):\n",
        "        for j in range(n_unlabelled):\n",
        "            max_list.append(X[i,:].reshape(1, -1)@X[j,:].reshape(1, -1).T)\n",
        "\n",
        "    for i in range(n_unlabelled):\n",
        "        for j in range(Y.shape[0]):\n",
        "            max_list.append(X[i,:].reshape(1, -1)@Y[j,:].reshape(1, -1).T)\n",
        "\n",
        "    gamma = np.max(max_list)\n",
        "    return gamma\n",
        "\n",
        "# get normalization for the relu kernel\n",
        "\n",
        "def contrastive_simple_ReLU_normalizer(data0, datam, datap):\n",
        "    n_unlabelled = data0.shape[0]\n",
        "    max_list = []\n",
        "    for i in range(n_unlabelled):\n",
        "        for j in range(n_unlabelled):\n",
        "            max_list.append(data0[i,:].reshape(1, -1)@data0[j,:].reshape(1, -1).T)\n",
        "\n",
        "            max_list.append(data0[i,:].reshape(1, -1)@datam[j,:].reshape(1, -1).T)\n",
        "            max_list.append(data0[i,:].reshape(1, -1)@datap[j,:].reshape(1, -1).T)\n",
        "\n",
        "            max_list.append(datap[i,:].reshape(1, -1)@datap[j,:].reshape(1, -1).T)\n",
        "            max_list.append(datam[i,:].reshape(1, -1)@datam[j,:].reshape(1, -1).T)\n",
        "            max_list.append(datap[i,:].reshape(1, -1)@datam[j,:].reshape(1, -1).T)\n",
        "            max_list.append(datam[i,:].reshape(1, -1)@datap[j,:].reshape(1, -1).T)\n",
        "\n",
        "    gamma = np.max(max_list)\n",
        "    return gamma\n",
        "\n",
        "##### split the data such that we obain train / test as well as positive an negative samples\n",
        "\n",
        "def create_data_split(X,Y, N_lab,N_test,random_state,noise):\n",
        "    # X: all data\n",
        "    # Y: all labels\n",
        "\n",
        "    X_train, X_test, Y_train, Y_test = train_test_split(X,Y, test_size=N_test, random_state=random_state)\n",
        "\n",
        "    # reference data point\n",
        "    X0 = X_train.copy()\n",
        "    # positive is reference + noise\n",
        "    Xp = X_train.copy() + noise * np.random.normal(0,1,X_train.shape)\n",
        "    # negative is indpendent sample\n",
        "    Xm = X_train.copy()\n",
        "    np.random.shuffle(Xm)\n",
        "\n",
        "    X_lab = X_train[:int(X.shape[0]*N_lab),:]\n",
        "    Y_lab = Y_train[:int(X.shape[0]*N_lab)]\n",
        "\n",
        "    return X_train,X_lab,Y_lab,X_test,Y_test,X0,Xp,Xm"
      ],
      "metadata": {
        "id": "_13E_QltfWiG"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qWDker6V0KO9"
      },
      "source": [
        "# Define Kernel Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "407QqMr40Okw"
      },
      "source": [
        "## Simple contrastive\n",
        "\n",
        "we follow directly the setting defined in Theorem 3:\n",
        "\n",
        "we use p,m as +,-.\n",
        "Deefine the Kernel matirces as\n",
        "$$\n",
        "\\begin{array}{ll}\n",
        "\\boldsymbol{K}=\\left[k\\left(x_i, x_j\\right)\\right]_{i, j} & \\boldsymbol{K}_{-}=\\left[k\\left(x_i, x_j^{-}\\right)\\right]_{i, j} \\\\\n",
        "\\boldsymbol{K}_{+}=\\left[k\\left(x_i, x_j^{+}\\right)\\right]_{i, j} & \\boldsymbol{K}_{--}=\\left[k\\left(x_i^{-}, x_j^{-}\\right)\\right]_{i, j} \\\\\n",
        "\\boldsymbol{K}_{++}=\\left[k\\left(x_i^{+}, x_j^{+}\\right)\\right]_{i, j} & \\boldsymbol{K}_{-+}=\\left[k\\left(x_i^{-}, \\boldsymbol{x}_j^{+}\\right)\\right]_{i, j}\n",
        "\\end{array}\n",
        "$$\n",
        "\n",
        "Furthermore, define the matrices\n",
        "$$\n",
        "\\begin{aligned}\n",
        "& K_{\\Delta}=K_{--}+K_{++}-K_{-+}-K_{-+}^T \\quad K_1=\\left[\\begin{array}{cc}\n",
        "\\boldsymbol{K} & \\boldsymbol{K}_{-}-\\boldsymbol{K}_{+} \\\\\n",
        "\\boldsymbol{K}_{-}-\\boldsymbol{K}_{+} & \\boldsymbol{K}_{\\Delta}\n",
        "\\end{array}\\right] \\\\\n",
        "& \\boldsymbol{B}=\\left[\\begin{array}{c}\n",
        "\\boldsymbol{K}_{-}-\\boldsymbol{K}_{+} \\\\\n",
        "\\boldsymbol{K}_{\\Delta}\n",
        "\\end{array}\\right] \\cdot\\left[\\begin{array}{ll}\n",
        "\\boldsymbol{K} & \\boldsymbol{K}_{-}-\\boldsymbol{K}_{+}\n",
        "\\end{array}\\right] \\quad \\boldsymbol{K}_2=-\\frac{1}{2}\\left(\\boldsymbol{B}+\\boldsymbol{B}^T\\right) \\\\\n",
        "&\n",
        "\\end{aligned}\n",
        "$$\n",
        "Let $A_2$ consist of the top h eigenvectors of the matrix $\\boldsymbol{K}_1^{-1 / 2} \\boldsymbol{K}_2 \\boldsymbol{K}_1^{-1 / 2}$ and $\\boldsymbol{A}=\\boldsymbol{K}_1^{-1 / 2} \\boldsymbol{A}_2$\n",
        "Then at optimal parameterization, the embedding of any $x^*$ can be written as\n",
        "$$\n",
        "z^*=A^T\\left[\\begin{array}{c}\n",
        "k\\left(x^*, \\boldsymbol{X}\\right) \\\\\n",
        "k\\left(x^*, \\boldsymbol{X}^{-}\\right)-k\\left(x^*, \\boldsymbol{X}^{+}\\right)\n",
        "\\end{array}\\right]\n",
        "$$"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "TZRyzgdl0KCE"
      },
      "outputs": [],
      "source": [
        "#### Function computting the embedding for the train and test set for the\n",
        "# simple contrastive kernel\n",
        "\n",
        "def compute_contrastive_simple_embedding(data0, datam, datap, dataLab, dataTest,\n",
        "                                         gamma, k, kernel = 'Gaussian'):\n",
        "\n",
        "    # data0, datam, datap: reference, negative and positive samples\n",
        "    # dataLab, dataTest: labeled and test data\n",
        "    # gamma: kernel parameterizaiton\n",
        "    # k: hidden dimension\n",
        "    # kernel: define kernel name\n",
        "\n",
        "    n_unlabelled = data0.shape[0]\n",
        "\n",
        "    if kernel == 'ReLU':\n",
        "        gamma = contrastive_simple_ReLU_normalizer(data0, datam, datap)\n",
        "\n",
        "    K = kernel_function(kernel,data0,data0,gamma = gamma)\n",
        "    K_m = kernel_function(kernel,data0,datam,gamma = gamma)\n",
        "    K_p = kernel_function(kernel,data0,datap,gamma = gamma)\n",
        "    K_pp = kernel_function(kernel,datap,datap,gamma = gamma)\n",
        "    K_mm = kernel_function(kernel,datam,datam,gamma = gamma)\n",
        "    K_mp = kernel_function(kernel,datam,datap,gamma = gamma)\n",
        "\n",
        "    K_Delta = K_mm + K_pp - K_mp - K_mp.T\n",
        "\n",
        "    K1 = np.block([[K,          K_m - K_p   ],\n",
        "                   [K_m - K_p,  K_Delta     ]])\n",
        "\n",
        "    B = np.block([[K_m - K_p],\n",
        "                  [K_Delta ]]) @ np.block([[K, K_m - K_p]])\n",
        "\n",
        "    K_2 = -0.5*(B + B.T)\n",
        "\n",
        "    K_1_sqrt = sqrtm(K1)\n",
        "\n",
        "    evals_large, evecs_large = largest_eigh(K_1_sqrt @ K_2 @ K_1_sqrt, subset_by_index=(n_unlabelled-k,n_unlabelled-1))\n",
        "    A_2 = np.real(evecs_large)\n",
        "\n",
        "    A = K_1_sqrt @ A_2\n",
        "\n",
        "    test_embed = A.T @ np.block([[[kernel_function(kernel,dataTest,data0,gamma = gamma).T],\n",
        "                                  [np.array(kernel_function(kernel,dataTest,datam,gamma = gamma) - kernel_function(kernel,dataTest,datap,gamma = gamma)).T]]])\n",
        "\n",
        "    train_embed = A.T @ np.block([[[kernel_function(kernel,dataLab,data0,gamma = gamma).T],\n",
        "                                  [np.array(kernel_function(kernel,dataLab,datam,gamma = gamma) - kernel_function(kernel,dataLab,datap,gamma = gamma)).T]]])\n",
        "\n",
        "    return np.squeeze(train_embed.real).T, np.squeeze(test_embed.real).T"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "07YRi_Oe0T8f"
      },
      "source": [
        "## Spectral contrastive\n",
        "\n",
        "Recall we compute the gradients as\n",
        "$$\n",
        "\\nabla \\mathcal{L}^{\\mathrm{Sp}}=2 \\lambda \\boldsymbol{Z} \\boldsymbol{K}^{-1}+ \\begin{cases}-2 \\boldsymbol{z}_{i+n}+2\\left(\\boldsymbol{z}_i^T \\boldsymbol{z}_{i+2 n}\\right) \\boldsymbol{z}_{i+2 n} & , i \\in[n] \\\\ -2 \\boldsymbol{z}_{i-n} & , i \\in[n+1,2 n] \\\\ 2\\left(\\boldsymbol{z}_i^T \\boldsymbol{z}_{i-2 n}\\right) z_{i-2 n} & , i \\in[2 n+1,3 n]\\end{cases}\n",
        "$$\n",
        "\n",
        "and we can compute a new point using\n",
        "$$\n",
        "\\boldsymbol{z}^*:=\\boldsymbol{Z} \\boldsymbol{K}^{-1} k\\left(\\boldsymbol{X}, \\boldsymbol{x}^*\\right).\n",
        "$$\n",
        "\n",
        "Remark: on thing to observe is that this setup need very very small learning rates and lambda. also if you run over differnt gammas you will see that in a lot of cases the caluclation breaks and gives a lot of error messages. However when you plot the loss, if you find a good gamma it converges\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_KDjL6Zj0S_y"
      },
      "outputs": [],
      "source": [
        "def spectral_loss(Z,K_inv,lambda_):\n",
        "    # compute spectral loss function\n",
        "    # not directly used but can be used to check if loss decreases\n",
        "    n = int(Z.shape[1]/3)\n",
        "    loss = 1/n * np.sum(-2*np.diag(Z[:,:n].T @ Z[:,n:2*n]) + (np.diag(Z[:,:n].T @ Z[:,2*n:]))**2) + lambda_*np.trace(Z @ K_inv @ Z.T)\n",
        "    return loss\n",
        "\n",
        "def update_spectral_loss(Z,K_inv,lambda_,lr):\n",
        "    # compute gradient step for the spectral loss.\n",
        "    # updates are as in Th.1.3, stacked\n",
        "    n = int(Z.shape[1]/3)\n",
        "    term_1 = 2 * lambda_ * Z @ K_inv\n",
        "    term_2 = np.hstack((-2*Z[:,n:2*n] + 2*(np.diag(Z[:,:n].T @ Z[:,2*n:]).T) * Z[:,2*n:],\n",
        "                       -2*Z[:,:n],\n",
        "                       2*(np.diag(Z[:,2*n:].T @ Z[:,:n]).T) * Z[:,:n]\n",
        "                       ))\n",
        "    return lr*( term_1 + term_2)\n",
        "\n",
        "def compute_contrastive_spectral_embedding(data0, datam, datap, dataLab, dataTest,\n",
        "                                           gamma ,kernel, hidden_size = 2, lambda_  = 1.0e-3,\n",
        "                                           lr = 1.0e-10, epochs = 100):\n",
        "\n",
        "    # data0, datam, datap: reference, negative and positive samples\n",
        "    # dataLab, dataTest: labeled and test data\n",
        "    # gamma: kernel parameterizaiton\n",
        "    # hidden_size: hidden dimension\n",
        "    # kernel: define kernel name\n",
        "    # lambda_: regulariser\n",
        "    # lr: learning rate\n",
        "    # epochs: number of epochs\n",
        "\n",
        "    X_all = np.concatenate((data0,datap,datam), axis = 0)\n",
        "    K_inv = np.linalg.pinv(kernel_function(kernel, X_all, X_all, gamma))\n",
        "    Z = np.random.normal(0,0.1,(hidden_size, data0.shape[0]*3))\n",
        "\n",
        "    loss_list = []\n",
        "    for epoch in range(epochs):\n",
        "        loss = spectral_loss(Z,K_inv,lambda_)\n",
        "        loss_list.append(loss)\n",
        "        Z = Z - update_spectral_loss(Z,K_inv,lambda_,lr)\n",
        "\n",
        "    embedd_test = (Z @ K_inv @ np.linalg.pinv(kernel_function(kernel, dataTest, X_all, gamma))).T\n",
        "    embedd_train = (Z @ K_inv @ np.linalg.pinv(kernel_function(kernel, dataLab, X_all, gamma))).T\n",
        "\n",
        "    scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(embedd_train)\n",
        "    embedd_train = scaling.transform(embedd_train)\n",
        "    embedd_test = scaling.transform(embedd_test)\n",
        "\n",
        "    return embedd_train,  embedd_test, loss_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5EunhP4I0fxj"
      },
      "source": [
        "## Kernel AE Reconstruction"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0WXS55o-0IDP"
      },
      "outputs": [],
      "source": [
        "def Zstep(Q, q, gamma, kernel):\n",
        "    # update embedding\n",
        "    if kernel == 'ReLU': gamma = ReLU_Kernel_normalize(Q)\n",
        "    K_Q = kernel_function(kernel, Q, Q, gamma)\n",
        "    _, eigenvectors = np.linalg.eigh(K_Q)\n",
        "    eigenvectors_descending = eigenvectors[:, ::-1]\n",
        "    Z = eigenvectors_descending[:, :q]\n",
        "\n",
        "    return(Z)\n",
        "\n",
        "def Qstep(Z, X, gamma, kernel):\n",
        "    # update reconstruction\n",
        "    if kernel == 'ReLU': gamma = ReLU_Kernel_normalize(Z)\n",
        "    K_Z = kernel_function(kernel, Z, Z, gamma)\n",
        "    Q = K_Z @ np.linalg.inv(K_Z + np.eye(np.shape(X)[0])) @ X\n",
        "\n",
        "    return(Q)\n",
        "\n",
        "def optimize_Bottleneck_Kernel_AE_iterative(X_train, X_test, X_lab,epochs,\n",
        "                                            kernel,gamma,embeddin_size,\n",
        "                                            return_embed = True,noise = 0):\n",
        "\n",
        "    # function can be used either to obtain the embeddings or de-noising\n",
        "    # X_train, dataLab, dataTest: train, labeled and test data\n",
        "    # gamma: kernel parameterizaiton\n",
        "    # embeddin_size: hidden dimension\n",
        "    # kernel: define kernel name\n",
        "    # return_embed if true return embedding, if not return\n",
        "    # noise if set to zero we have a standard AE setting if not de-nosing\n",
        "\n",
        "    np.random.seed(0)\n",
        "    X_test_Noise = X_test.copy() + noise * np.random.normal(0,1,X_test.shape)\n",
        "    X_train_Noise = X_train.copy() + noise * np.random.normal(0,1,X_train.shape)\n",
        "    X_lab_Noise = X_lab.copy() + noise * np.random.normal(0,1,X_lab.shape)\n",
        "\n",
        "    train_loss = [ ]\n",
        "\n",
        "    Q_train = X_train\n",
        "    for _ in range(epochs):\n",
        "        Z = Zstep(Q_train, embeddin_size ,gamma, kernel)\n",
        "        Q_train = Qstep(Z, X_train, gamma, kernel)\n",
        "\n",
        "        loss = np.linalg.norm(Q_train-X_train)**2 / torch.tensor(X_train).shape[0]\n",
        "        train_loss.append(loss)\n",
        "\n",
        "\n",
        "    if return_embed:\n",
        "        if kernel == 'ReLU': gamma = ReLU_Kernel_normalize_2(X_train,X_test)\n",
        "        Out_test = (Z.T @ np.linalg.pinv(kernel_function(kernel,X_train,X_train,gamma)) @ kernel_function(kernel,X_train, X_test,gamma)).T\n",
        "\n",
        "        if kernel == 'ReLU': gamma = ReLU_Kernel_normalize_2(X_train,X_lab)\n",
        "        Out_lab = (Z.T @ np.linalg.pinv(kernel_function(kernel,X_train,X_train,gamma)) @ kernel_function(kernel,X_train,X_lab,gamma)).T\n",
        "\n",
        "    else:\n",
        "        if kernel == 'ReLU': gamma = ReLU_Kernel_normalize_2(X_train,X_test)\n",
        "        Z_new = Z.T @ np.linalg.inv(kernel_function(kernel, X_train_Noise, X_train_Noise, gamma)) @ kernel_function(kernel, X_train_Noise, X_test_Noise, gamma)\n",
        "        K_Z = kernel_function(kernel, Z, Z, gamma)\n",
        "        K_Z_new = kernel_function(kernel, Z, Z_new.T, gamma)\n",
        "        Q_test = K_Z_new.T @ np.linalg.inv(K_Z + np.eye(np.shape(K_Z)[0])) @ X_train\n",
        "        Out_test = np.linalg.norm(Q_test-X_test)**2 / torch.tensor(X_train).shape[0]\n",
        "\n",
        "        if kernel == 'ReLU': gamma = ReLU_Kernel_normalize_2(X_train,X_lab)\n",
        "        Z_new = Z.T @ np.linalg.inv(kernel_function(kernel, X_train_Noise, X_train_Noise, gamma)) @ kernel_function(kernel, X_train_Noise, X_lab_Noise, gamma)\n",
        "        K_Z = kernel_function(kernel, Z, Z, gamma)\n",
        "        K_Z_new = kernel_function(kernel, Z, Z_new.T, gamma)\n",
        "        Q_lab = K_Z_new.T @ np.linalg.inv(K_Z + np.eye(np.shape(K_Z)[0])) @ X_train\n",
        "        Out_lab = np.linalg.norm(Q_lab-X_lab)**2 / torch.tensor(X_train).shape[0]\n",
        "\n",
        "\n",
        "    return train_loss, Out_lab,  Out_test"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Neural Networks\n",
        "\n",
        "Define the neural network models corresponding to the proposed kernel methods. the setup are standard relu neural networks with the considered contrastive loss functions, AE as well as a neural network performing classification directly on the labelled data."
      ],
      "metadata": {
        "id": "qBvXSL0Ly_68"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Contrastive"
      ],
      "metadata": {
        "id": "C3xTIk_VzKbV"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU_NN_orthogonal(nn.Module):\n",
        "    def __init__(self, in_feature, embed):\n",
        "        super(ReLU_NN_orthogonal, self).__init__()\n",
        "        # encoder\n",
        "        self.enc1 = torch.nn.utils.parametrizations.orthogonal(\n",
        "            nn.Linear(in_features=in_feature, out_features=embed, bias=False))\n",
        "        # decoder\n",
        "        self.dec1 = torch.nn.utils.parametrizations.orthogonal(\n",
        "            nn.Linear(in_features=embed, out_features=1, bias=False))\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "class ReLU_NN(nn.Module):\n",
        "    def __init__(self, in_feature, embed):\n",
        "        super(ReLU_NN, self).__init__()\n",
        "        # encoder\n",
        "        self.enc1 = nn.Linear(in_features=in_feature, out_features=embed, bias=False)\n",
        "        # decoder\n",
        "        self.dec1 = nn.Linear(in_features=embed, out_features=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def simple_contrastive_loss(f0, fp, fm):\n",
        "    return torch.mean(f0.T@(fm-fp))\n",
        "\n",
        "def spectral_contrastive_loss(f0, fp, fm):\n",
        "    return torch.mean(-2*(f0.T@fm) + (f0.T@fp)**2)\n",
        "\n",
        "def contrastive_NN_train(X0, Xp, Xm, dataLab, Xtest, embedding_size,\n",
        "                         contrastive_loss = 'simple', lr=0.1, epochs=100):\n",
        "\n",
        "    # contrastive_loss: pass either 'simple' or 'spectral' to change the way the loss is defined\n",
        "\n",
        "    X0 = torch.from_numpy(X0).to(device)\n",
        "    Xp = torch.from_numpy(Xp).to(device)\n",
        "    Xm = torch.from_numpy(Xm).to(device)\n",
        "\n",
        "    if contrastive_loss == 'simple':\n",
        "        net = ReLU_NN_orthogonal(in_feature=X0.shape[1],embed=embedding_size).to(device)\n",
        "    elif contrastive_loss == 'spectral':\n",
        "        net = ReLU_NN(in_feature=X0.shape[1],embed=embedding_size).to(device)\n",
        "\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "\n",
        "    train_loss = []\n",
        "    out_diff = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        out0 = net(X0.float())\n",
        "        outp = net(Xp.float())\n",
        "        outm = net(Xm.float())\n",
        "        if contrastive_loss == 'simple':\n",
        "            total_loss = simple_contrastive_loss(out0, outp, outm)\n",
        "        elif contrastive_loss == 'spectral':\n",
        "            total_loss = spectral_contrastive_loss(out0, outp, outm)\n",
        "        else:\n",
        "            print('Neural network loss not defined')\n",
        "        train_loss.append(total_loss.item())\n",
        "        total_loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    NN_dataLab_embed = net(torch.tensor(dataLab).to(device).float()).detach().cpu().numpy()\n",
        "    NN_Xtestembed = net(torch.tensor(Xtest).to(device).float()).detach().cpu().numpy()\n",
        "\n",
        "    return NN_dataLab_embed, NN_Xtestembed"
      ],
      "metadata": {
        "id": "1DvRRdFVy4ib"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## AE\n",
        "simple AE with relu activation functions"
      ],
      "metadata": {
        "id": "BDAfVwXOzR-D"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Get embedding"
      ],
      "metadata": {
        "id": "VVa-k1id8N0I"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class Bottleneck_Autoencoder(nn.Module):\n",
        "    def __init__(self, in_feature, hidden, embed):\n",
        "        super(Bottleneck_Autoencoder, self).__init__()\n",
        "\n",
        "        # encoder\n",
        "        self.enc1 = nn.Linear(in_features=in_feature, out_features=hidden, bias=False)\n",
        "        self.enc2 = nn.Linear(in_features=hidden, out_features=embed, bias=False)\n",
        "\n",
        "        # decoder\n",
        "        self.dec1 = nn.Linear(in_features=embed, out_features=hidden, bias=False)\n",
        "        self.dec2 = nn.Linear(in_features=hidden, out_features=in_feature, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.enc2(x)\n",
        "        x = self.dec1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dec2(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def train_AE(net, X,X_noise, lr=0.1, epochs=100):\n",
        "\n",
        "\n",
        "    X = torch.tensor(X).to(device)\n",
        "    X_noise = torch.tensor(X_noise).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    train_loss = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = net(X_noise.float())\n",
        "        loss = criterion(out, X.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    return train_loss, net\n",
        "\n",
        "def bottleneck_AE_NN_train(X_lab,X_unlab,X_test,embed,hidden,lr,epochs):\n",
        "\n",
        "    net = Bottleneck_Autoencoder(in_feature=torch.tensor(X_unlab).shape[1],embed=embed,hidden = hidden).to(device)\n",
        "    train_loss,net = train_AE(net, torch.tensor(X_unlab).float(),torch.tensor(X_unlab).float(), lr=lr, epochs=epochs)\n",
        "\n",
        "    # get weigths so embedding can be computed.\n",
        "    W2 = net.enc2.weight.detach()\n",
        "    W1 = net.enc1.weight.detach()\n",
        "\n",
        "    # compute embeddings and move them to numpy arrays\n",
        "    embed_lab = F.relu(torch.tensor(X_lab).float() @ W1.T) @ W2.T\n",
        "    embed_lab = embed_lab.detach().cpu().numpy()\n",
        "\n",
        "    embed_test = F.relu(torch.tensor(X_test).float() @ W1.T) @ W2.T\n",
        "    embed_test = embed_test.detach().cpu().numpy()\n",
        "\n",
        "    return embed_lab, embed_test"
      ],
      "metadata": {
        "id": "MMba8mNKzUKT"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### de-noising AE"
      ],
      "metadata": {
        "id": "8OtCzbBB8Qvt"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def train_DAE(net, X,X_noise, lr=0.1, epochs=100):\n",
        "\n",
        "\n",
        "    X = torch.tensor(X).to(device)\n",
        "    X_noise = torch.tensor(X_noise).to(device)\n",
        "\n",
        "    criterion = nn.MSELoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    train_loss = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = net(X_noise.float())\n",
        "        loss = criterion(out, X.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "        train_loss.append(loss.item())\n",
        "\n",
        "    return train_loss, net\n",
        "\n",
        "def bottleneck_DAE_NN_train(X_lab,X_unlab,X_test,noise,embed,hidden,lr,epochs):\n",
        "\n",
        "    np.random.seed(0)\n",
        "\n",
        "    X_lab_Noise = X_lab.copy() + noise * np.random.normal(0,1,X_lab.shape)\n",
        "    X_unlab_Noise = X_unlab.copy() + noise * np.random.normal(0,1,X_unlab.shape)\n",
        "    X_test_Noise = X_test.copy() + noise * np.random.normal(0,1,X_test.shape)\n",
        "\n",
        "    net = Bottleneck_Autoencoder(in_feature=torch.tensor(X_unlab).shape[1],embed=embed,hidden = hidden).to(device)\n",
        "    train_loss,net = train_DAE(net, torch.tensor(X_unlab).float(),torch.tensor(X_unlab_Noise).float(), lr=lr, epochs=epochs)\n",
        "\n",
        "    out_test = net(torch.tensor(X_test_Noise).float()).detach().cpu().numpy()\n",
        "    test_loss = np.linalg.norm(out_test-X_test)**2 / torch.tensor(X_test).shape[0]\n",
        "\n",
        "    out_val = net(torch.tensor(X_lab_Noise).float()).detach().cpu().numpy()\n",
        "    val_loss = np.linalg.norm(out_val-X_lab)**2 / torch.tensor(X_lab).shape[0]\n",
        "\n",
        "    return train_loss, val_loss, test_loss"
      ],
      "metadata": {
        "id": "mH_yYQO1ISkR"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Classification\n",
        "\n",
        "Simple classification network as benchmark"
      ],
      "metadata": {
        "id": "HblPRSkpzfkp"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "class ReLU_NN_classification(nn.Module):\n",
        "    def __init__(self, in_feature, hidden):\n",
        "        super(ReLU_NN_classification, self).__init__()\n",
        "        self.enc1 = nn.Linear(in_features=in_feature, out_features=hidden, bias=False)\n",
        "        self.dec1 = nn.Linear(in_features=hidden, out_features=1, bias=False)\n",
        "\n",
        "    def forward(self, x):\n",
        "        x = self.enc1(x)\n",
        "        x = F.relu(x)\n",
        "        x = self.dec1(x)\n",
        "\n",
        "        return x\n",
        "\n",
        "def accuracy(out, labels):\n",
        "    _,pred = torch.max(out, dim=1)\n",
        "    return torch.sum(pred==labels).item()\n",
        "\n",
        "\n",
        "def train_classification_NN(X_train,Y_train,X_test,Y_test,\n",
        "                            hidden = 200, lr=0.1, epochs=100):\n",
        "\n",
        "    net = ReLU_NN_classification(in_feature=X_train.shape[1],\n",
        "                                 hidden=hidden).to(device)\n",
        "\n",
        "    X_train = torch.from_numpy(X_train).to(device)\n",
        "    Y_train = torch.from_numpy(Y_train).to(device)\n",
        "    X_test = torch.from_numpy(X_test).to(device)\n",
        "    Y_test = torch.from_numpy(Y_test).to(device)\n",
        "\n",
        "    criterion = nn.CrossEntropyLoss()\n",
        "    optimizer = optim.SGD(net.parameters(), lr=lr)\n",
        "    train_loss = []\n",
        "\n",
        "    for epoch in range(epochs):\n",
        "        optimizer.zero_grad()\n",
        "        out = net(X_train.float())\n",
        "        loss = criterion(torch.squeeze(out).float(), Y_train.float())\n",
        "        loss.backward()\n",
        "        optimizer.step()\n",
        "\n",
        "    with torch.no_grad():\n",
        "        net.eval()\n",
        "        outputs_t = net(X_test.float())\n",
        "        _,pred_t = torch.max(outputs_t, dim=1)\n",
        "        correct_t = torch.sum(pred_t==Y_test).item()\n",
        "        total_t = Y_test.size(0)\n",
        "    print(correct_t,correct_t/total_t)\n",
        "\n",
        "    return correct_t/total_t"
      ],
      "metadata": {
        "id": "eAHf7JoYzh-7"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "def run_de_noising_experiment(X,Y,noise,N_lab,N_test,rep,gamma_list,data_set_name,epochs = 200):\n",
        "\n",
        "    all_mse_list = []\n",
        "\n",
        "    for r in range(rep):\n",
        "        print(r)\n",
        "\n",
        "        rep_temp = []\n",
        "\n",
        "        X_train,X_lab,Y_lab,X_test,Y_test,X0,Xp,Xm = create_data_split(X,Y, N_lab,N_test,r,noise=0.1)\n",
        "\n",
        "        _, _, test_loss = bottleneck_DAE_NN_train(X_lab,X_train,X_test,noise,embed=100,hidden=5,lr=0.1,epochs=epochs)\n",
        "        rep_temp.append(test_loss)\n",
        "\n",
        "        for kernel in ['Linear', 'ReLU']:\n",
        "            print(kernel)\n",
        "            try:\n",
        "                _, _, test_loss = optimize_Bottleneck_Kernel_AE_iterative(X_train, X_test,X_lab,noise,epochs=epochs,kernel=kernel,gamma=1,embeddin_size=5,\n",
        "                                                                          return_embed=False,noise = 0.1)\n",
        "                rep_temp.append(test_loss)\n",
        "            except:\n",
        "                rep_temp.append(np.nan)\n",
        "\n",
        "        for kernel in ['Gaussian','Laplacian']:\n",
        "            print(kernel)\n",
        "            temp_val = []\n",
        "            temp_test = []\n",
        "            for gamma in gamma_list:\n",
        "                try:\n",
        "                    _, val_loss, test_loss = optimize_Bottleneck_Kernel_AE_iterative(X_train, X_test,X_lab,noise,epochs=epochs,kernel=kernel,gamma=1,embeddin_size=5,\n",
        "                                                                                     return_embed=False,noise = 0.1)\n",
        "                    temp_val.append(val_loss)\n",
        "                    temp_test.append(test_loss)\n",
        "                except:\n",
        "                    temp_val.append(np.nan)\n",
        "                    temp_test.append(np.nan)\n",
        "\n",
        "            rep_temp.append(np.array(temp_test)[np.argmin(np.array(temp_val))])\n",
        "\n",
        "        all_mse_list.append(rep_temp)\n",
        "    np.save('de_noising/de_noising_{}'.format(data_set_name),np.array(all_mse_list))\n",
        "    return(all_mse_list)\n"
      ],
      "metadata": {
        "id": "6w5UWxBoJZD3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Run NN\n"
      ],
      "metadata": {
        "id": "IfnkR2VS1Vt9"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# Get SVM and KNN accuracy for a given labeled dataset and test\n",
        "\n",
        "def SVM_KNN(X_lab, Y_lab, X_test,Y_test):\n",
        "    try:\n",
        "        neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "        neigh.fit(X_lab, Y_lab)\n",
        "        neigh_predict = neigh.predict(X_test)\n",
        "        neigh_predict_accuracy = accuracy_score(Y_test, neigh_predict)\n",
        "    except:\n",
        "        neigh_predict_accuracy = 0\n",
        "\n",
        "    # sometimes SVM computation breaks\n",
        "    try:\n",
        "        clf = svm.SVC(kernel='linear')\n",
        "        clf.fit(X_lab,Y_lab)\n",
        "        SVM_predict = clf.predict(X_test)\n",
        "        SVM_predict_accuracy = accuracy_score(Y_test, SVM_predict)\n",
        "    except:\n",
        "        SVM_predict_accuracy = 0\n",
        "\n",
        "    return neigh_predict_accuracy, SVM_predict_accuracy"
      ],
      "metadata": {
        "id": "E66elzWk2MdD"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "function that runs over all NN models and stores K-nn and SVM accuracies for on the embeddings"
      ],
      "metadata": {
        "id": "dXnWgGxSETVg"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "def run_NN(X,Y,N_test,N_lab,rep,scale=False,embedding_size = 2):\n",
        "    all_benchmark_list = []\n",
        "\n",
        "    for r in range(rep):\n",
        "\n",
        "        accuracy_list = []\n",
        "\n",
        "        # get data splits\n",
        "        X_train,X_lab,Y_lab,X_test,Y_test,X0,Xp,Xm = create_data_split(X,Y, N_lab,N_test,r,noise=0.1)\n",
        "\n",
        "        ## NN reference clasefier\n",
        "\n",
        "        class_NN = train_classification_NN(X_lab,Y_lab,X_test,Y_test, hidden = 20, lr=0.1, epochs=100)\n",
        "        accuracy_list.append(class_NN)\n",
        "\n",
        "        # NN simple contrastive\n",
        "\n",
        "        embed_lab, embed_test = contrastive_NN_train(X0, Xp, Xm, X_lab, X_test, embedding_size, contrastive_loss = 'simple', lr=0.1, epochs=100)\n",
        "\n",
        "        neigh_predict_accuracy, SVM_predict_accuracy = SVM_KNN(embed_lab, Y_lab, embed_test,Y_test)\n",
        "        accuracy_list.append(neigh_predict_accuracy)\n",
        "        accuracy_list.append(SVM_predict_accuracy)\n",
        "\n",
        "        # NN spectral contrastive\n",
        "\n",
        "        embed_lab, embed_test = contrastive_NN_train(X0, Xp, Xm, X_lab, X_test, embedding_size, contrastive_loss = 'spectral', lr=0.1, epochs=100)\n",
        "\n",
        "        neigh_predict_accuracy, SVM_predict_accuracy = SVM_KNN(embed_lab, Y_lab, embed_test,Y_test)\n",
        "        accuracy_list.append(neigh_predict_accuracy)\n",
        "        accuracy_list.append(SVM_predict_accuracy)\n",
        "\n",
        "        # NN AE\n",
        "\n",
        "        embed_lab, embed_test = bottleneck_AE_NN_train(X_lab,X_train,X_test,embed=2,hidden = 200, lr=0.1, epochs=100)\n",
        "\n",
        "        neigh_predict_accuracy, SVM_predict_accuracy = SVM_KNN(embed_lab, Y_lab, embed_test,Y_test)\n",
        "        accuracy_list.append(neigh_predict_accuracy)\n",
        "        accuracy_list.append(SVM_predict_accuracy)\n",
        "\n",
        "\n",
        "        all_benchmark_list.append(accuracy_list)\n",
        "\n",
        "    np.save('embeddings/all_NN_list{}_{}_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),all_benchmark_list)\n",
        "\n"
      ],
      "metadata": {
        "id": "sjsqT8ui1Vbe"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "5gHgE9nC01pp"
      },
      "source": [
        "# Run all Kernel models and get embeddings"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Function that runs over Kernel PCA and all kernel methods.\n",
        "\n",
        "for Linear and ReLU kernel we run only once for every repetition.\n",
        "for gaussian and laplacian over all gamma.\n",
        "everything is stored in two different np files, that are then loaded for comuputing accuracy."
      ],
      "metadata": {
        "id": "yltmhlNpnEsF"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4tXzzPxh047O"
      },
      "outputs": [],
      "source": [
        "def create_embedddings(X,Y,N_lab,N_test,rep,gamma_list,data_set_name,scale = False,embedding_size=2):\n",
        "\n",
        "    def laplacian_kernel_manual(x,y):\n",
        "        return np.exp(-gamma*np.linalg.norm(x-y,1))\n",
        "\n",
        "    def kappa_0(u):\n",
        "        return 1/np.pi * (np.pi - np.arccos(u))\n",
        "\n",
        "    def kappa_1(u):\n",
        "        return 1/np.pi * (u*(np.pi - np.arccos(u)) + np.sqrt(1-u**2))\n",
        "\n",
        "    def ReLU_Kernel_PCA(x,y):\n",
        "        inp = (x@y.T)/gamma\n",
        "        if inp <-1: inp = -1\n",
        "        if inp >1: inp = 1\n",
        "        K_ij = inp*kappa_0(inp) + kappa_1(inp)\n",
        "        return K_ij\n",
        "\n",
        "    embed_test_list = []\n",
        "    embed_lab_list = []\n",
        "    embed_lin_test_list = []\n",
        "    embed_lin_lab_list = []\n",
        "    Y_lab_list = []\n",
        "    Y_test_list = []\n",
        "\n",
        "    if scale:\n",
        "        scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "        X = scaling.transform(X)\n",
        "\n",
        "    # run over repetions of different data splits\n",
        "    for r in range(rep):\n",
        "\n",
        "        # get data splits\n",
        "        X_train,X_lab,Y_lab,X_test,Y_test,X0,Xp,Xm = create_data_split(X,Y, N_lab,N_test,r,noise=0.1)\n",
        "        print('rep: {} || Train: {} Lab: {} Test: {} '.format(r,X_train.shape,X_lab.shape,X_test.shape))\n",
        "\n",
        "        Y_test_list.append(Y_test)\n",
        "        Y_lab_list.append(Y_lab)\n",
        "\n",
        "        ######################################### KERNEL PCA\n",
        "\n",
        "        print(' - Kernel PCA')\n",
        "        gamma = ReLU_Kernel_normalize(X_train)\n",
        "        for kernel in ['linear', ReLU_Kernel_PCA]:\n",
        "\n",
        "            kernel_pca = KernelPCA(n_components=embedding_size, kernel=kernel)\n",
        "            kernel_pca_train = kernel_pca.fit(X_train)\n",
        "\n",
        "            X_test_kernel_pca = kernel_pca_train.transform(X_test)\n",
        "            X_lab_kernel_pca = kernel_pca_train.transform(X_lab)\n",
        "            embed_lin_test_list.append(X_test_kernel_pca)\n",
        "            embed_lin_lab_list.append(X_lab_kernel_pca)\n",
        "\n",
        "        kernel_name_list = ['rbf', laplacian_kernel_manual]\n",
        "\n",
        "        for kernel in kernel_name_list:\n",
        "            test_temp = []\n",
        "            lab_temp = []\n",
        "            for g in gamma_list:\n",
        "\n",
        "                kernel_pca = KernelPCA(n_components=embedding_size, kernel=kernel,gamma = g)\n",
        "                try:\n",
        "                    kernel_pca_train = kernel_pca.fit(X_train)\n",
        "                    X_test_kernel_pca = kernel_pca_train.transform(X_test)\n",
        "                    X_lab_kernel_pca = kernel_pca_train.transform(X_lab)\n",
        "                except:\n",
        "                    print(\"PCA computation error for gamma {}\".format(g))\n",
        "                    X_test_kernel_pca = np.zeros((X_test.shape[0],embedding_size))\n",
        "                    X_lab_kernel_pca =  np.zeros((X_lab.shape[0],embedding_size))\n",
        "                test_temp.append(X_test_kernel_pca)\n",
        "                lab_temp.append(X_lab_kernel_pca)\n",
        "            embed_test_list.append(test_temp)\n",
        "            embed_lab_list.append(lab_temp)\n",
        "\n",
        "        ######################################### SIMPLE CONTRASTIVE\n",
        "\n",
        "        print(' - simple contrastive')\n",
        "        for kernel in ['Linear', 'ReLU']:\n",
        "            try:\n",
        "                Z_lab, Z_test = compute_contrastive_simple_embedding(X0, Xm, Xp, X_lab, X_test, g, k=embedding_size, kernel = kernel)\n",
        "            except:\n",
        "                print(\"spectral error for {}\".format(kernel))\n",
        "                Z_test = np.zeros((X_test.shape[0],embedding_size))\n",
        "                Z_lab = np.zeros((X_lab.shape[0],embedding_size))\n",
        "            embed_lin_test_list.append(Z_test)\n",
        "            embed_lin_lab_list.append(Z_lab)\n",
        "\n",
        "        kernel_name_list = ['Gaussian','Laplacian']\n",
        "        for kernel in kernel_name_list:\n",
        "            test_temp = []\n",
        "            lab_temp = []\n",
        "            for g in gamma_list:\n",
        "                try:\n",
        "                    Z_lab,  Z_test = compute_contrastive_simple_embedding(X0, Xm, Xp, X_lab, X_test, g, k=embedding_size, kernel = kernel)\n",
        "                except:\n",
        "                    print(\"simple contrastive for gamma {}\".format(g))\n",
        "                    Z_test = np.zeros((X_test.shape[0],embedding_size))\n",
        "                    Z_lab = np.zeros((X_lab.shape[0],embedding_size))\n",
        "                test_temp.append(Z_test)\n",
        "                lab_temp.append(Z_lab)\n",
        "            embed_test_list.append(test_temp)\n",
        "            embed_lab_list.append(lab_temp)\n",
        "\n",
        "\n",
        "        ######################################### SPECTRAL CONTRASTIVE\n",
        "\n",
        "        print(' - spectral contrastive')\n",
        "        lambda_  = 1.0e-5\n",
        "        lr =  1.0e-10\n",
        "        epochs = 1000\n",
        "\n",
        "        for kernel in ['Linear', 'ReLU']:\n",
        "            try:\n",
        "                Z_lab, Z_test, _ = compute_contrastive_spectral_embedding(X0, Xm, Xp, X_lab, X_test,\n",
        "                                                    gamma=g ,kernel = kernel, hidden_size = embedding_size,lambda_  = lambda_, lr =  lr,epochs = epochs)\n",
        "            except:\n",
        "                print(\"spectral error for {}\".format(kernel))\n",
        "                Z_test = np.zeros((X_test.shape[0],embedding_size))\n",
        "                Z_lab = np.zeros((X_lab.shape[0],embedding_size))\n",
        "            embed_lin_test_list.append(Z_test)\n",
        "            embed_lin_lab_list.append(Z_lab)\n",
        "\n",
        "        kernel_name_list = ['Gaussian','Laplacian']\n",
        "        for kernel in kernel_name_list:\n",
        "            test_temp = []\n",
        "            lab_temp = []\n",
        "            for g in gamma_list:\n",
        "                try:\n",
        "                    Z_lab, Z_test, _ = compute_contrastive_spectral_embedding(X0, Xm, Xp, X_lab, X_test,\n",
        "                                                gamma=g ,kernel = kernel, hidden_size = embedding_size,lambda_  = lambda_, lr =  lr,epochs = epochs)\n",
        "                except:\n",
        "                    print(\"spectral contrastive error for gamma {}\".format(g))\n",
        "                    Z_test = np.zeros((X_test.shape[0],embedding_size))\n",
        "                    Z_lab = np.zeros((X_lab.shape[0],embedding_size))\n",
        "                test_temp.append(Z_test)\n",
        "                lab_temp.append(Z_lab)\n",
        "            embed_test_list.append(test_temp)\n",
        "            embed_lab_list.append(lab_temp)\n",
        "\n",
        "        ######################################### BOTTLENECK\n",
        "\n",
        "        print(' - bottleneck')\n",
        "        for kernel in ['Linear', 'ReLU']:\n",
        "            epochs = 20\n",
        "            gamma = 1\n",
        "            _, Z_lab,  Z_test = optimize_Bottleneck_Kernel_AE_iterative(X_train, X_test,X_lab,epochs=epochs,kernel=kernel,gamma=1,embeddin_size=embedding_size)\n",
        "\n",
        "            embed_lin_test_list.append(Z_test)\n",
        "            embed_lin_lab_list.append(Z_lab)\n",
        "\n",
        "        kernel_name_list = ['Gaussian','Laplacian']\n",
        "        for kernel in kernel_name_list:\n",
        "            test_temp = []\n",
        "            lab_temp = []\n",
        "            for g in gamma_list:\n",
        "\n",
        "                try:\n",
        "                    _, Z_lab, Z_test = optimize_Bottleneck_Kernel_AE_iterative(X_train, X_test,X_lab,epochs=epochs,\n",
        "                                                                            kernel=kernel,gamma=g,embeddin_size=embedding_size)\n",
        "                except:\n",
        "                    print('Bottleneck failed for gamma {}'.format(g))\n",
        "                    Z_test = np.zeros((X_test.shape[0],embedding_size))\n",
        "                    Z_lab=  np.zeros((X_lab.shape[0],embedding_size))\n",
        "\n",
        "                test_temp.append(Z_test)\n",
        "                lab_temp.append(Z_lab)\n",
        "            embed_test_list.append(test_temp)\n",
        "            embed_lab_list.append(lab_temp)\n",
        "\n",
        "\n",
        "    np.save('embeddings/embed{}_{}_lin_test_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),embed_lin_test_list)\n",
        "    np.save('embeddings/embed{}_{}_lin_lab_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),embed_lin_lab_list)\n",
        "    np.save('embeddings/embed{}_{}_test_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),embed_test_list)\n",
        "    np.save('embeddings/embed{}_{}_lab_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),embed_lab_list)\n",
        "    np.save('embeddings/Y_lab{}_{}_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),Y_lab_list)\n",
        "    np.save('embeddings/Y_test{}_{}_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),Y_test_list)\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "238lKW5M5VNo"
      },
      "source": [
        "# Downstream Task"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Leave one out valiation functions"
      ],
      "metadata": {
        "id": "bo8wvxPBndDi"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "SGQCwW2MIxRx"
      },
      "outputs": [],
      "source": [
        "def get_validation_Knn(y_lab,x_lab):\n",
        "    acc_list = []\n",
        "    loo = LeaveOneOut()\n",
        "    for i, (train_index, test_index) in enumerate(loo.split(x_lab)):\n",
        "        neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "        neigh.fit(x_lab[train_index,:],y_lab[train_index])\n",
        "        neigh_predict = neigh.predict(x_lab[test_index,:])\n",
        "        neigh_predict_accuracy = accuracy_score(y_lab[test_index], neigh_predict)\n",
        "        acc_list.append(neigh_predict_accuracy)\n",
        "    return np.mean(np.array(acc_list))\n",
        "\n",
        "def get_validation_SVM(y_lab,x_lab):\n",
        "    acc_list = []\n",
        "    loo = LeaveOneOut()\n",
        "    for i, (train_index, test_index) in enumerate(loo.split(x_lab)):\n",
        "        neigh = svm.SVC(kernel='linear')\n",
        "        neigh.fit(x_lab[train_index,:],y_lab[train_index])\n",
        "        neigh_predict = neigh.predict(x_lab[test_index,:])\n",
        "        neigh_predict_accuracy = accuracy_score(y_lab[test_index], neigh_predict)\n",
        "        acc_list.append(neigh_predict_accuracy)\n",
        "    return np.mean(np.array(acc_list))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "q33gxMX35XVo"
      },
      "outputs": [],
      "source": [
        "def choose_best_gamma(rep,gamma_list,data_set_name,scale = False,embedding_size = 2):\n",
        "\n",
        "    # Load embedings\n",
        "\n",
        "    num_kernel = 2\n",
        "    num_gamma = len(gamma_list)\n",
        "\n",
        "    SVM_test_list = []\n",
        "    Knn_test_list = []\n",
        "\n",
        "    embed_test_list = np.load('embeddings/embed{}_{}_test_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "    embed_lab_list = np.load('embeddings/embed{}_{}_lab_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "    Y_test_list = np.load('embeddings/Y_test{}_{}_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "    Y_lab_list = np.load('embeddings/Y_lab{}_{}_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "\n",
        "    print(embed_lab_list.shape)\n",
        "\n",
        "    # run over saved embeddings and for each list of gammas use leave on oute\n",
        "    # validation to pick the parameterisation for test\n",
        "\n",
        "    idx = 0\n",
        "    for r in range(rep):\n",
        "        y_lab = Y_lab_list[r,:]\n",
        "        y_test = Y_test_list[r,:]\n",
        "        temp_SVM_test_list = []\n",
        "        temp_Knn_test_list = []\n",
        "\n",
        "        for i in range(num_kernel*4):\n",
        "\n",
        "            ######### KNN\n",
        "            gamma_val_list = []\n",
        "            for j in range(num_gamma):\n",
        "                x_lab = embed_lab_list[idx,j,:,:]\n",
        "                try:\n",
        "                    val_accuracy = get_validation_Knn(y_lab,x_lab)\n",
        "                except:\n",
        "                    val_accuracy = 0\n",
        "                gamma_val_list.append(val_accuracy)\n",
        "\n",
        "            try:\n",
        "                max_idx = np.argmax(np.array(gamma_val_list))\n",
        "                neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "                neigh.fit(embed_lab_list[idx,max_idx,:,:],y_lab)\n",
        "                neigh_predict = neigh.predict(embed_test_list[idx,max_idx,:,:])\n",
        "                neigh_predict_accuracy = accuracy_score(y_test, neigh_predict)\n",
        "                temp_Knn_test_list.append(neigh_predict_accuracy)\n",
        "            except:\n",
        "                temp_Knn_test_list.append(0)\n",
        "\n",
        "            ######### SVM\n",
        "            gamma_val_list = []\n",
        "\n",
        "            for j in range(num_gamma):\n",
        "                x_lab = embed_lab_list[idx,j,:,:]\n",
        "                try:\n",
        "                    val_accuracy = get_validation_SVM(y_lab,x_lab)\n",
        "                except:\n",
        "                    val_accuracy = 0\n",
        "                gamma_val_list.append(val_accuracy)\n",
        "\n",
        "            try:\n",
        "                max_idx = np.argmax(np.array(gamma_val_list))\n",
        "                neigh = svm.SVC(kernel='linear')\n",
        "                neigh.fit(embed_lab_list[idx,max_idx,:,:],y_lab)\n",
        "                neigh_predict = neigh.predict(embed_test_list[idx,max_idx,:,:])\n",
        "                neigh_predict_accuracy = accuracy_score(y_test, neigh_predict)\n",
        "                temp_SVM_test_list.append(neigh_predict_accuracy)\n",
        "            except:\n",
        "                temp_SVM_test_list.append(0)\n",
        "\n",
        "            idx+=1\n",
        "\n",
        "        Knn_test_list.append(temp_Knn_test_list)\n",
        "        SVM_test_list.append(temp_SVM_test_list)\n",
        "\n",
        "    all_test_list = np.array([Knn_test_list, SVM_test_list])\n",
        "    np.save('embeddings/all_test_list{}_{}_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),all_test_list)\n",
        "\n",
        "    # return SVM_test_list,Knn_test_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "compute downstream for tasks, that do not have hyperparameters and therefore dont need valiation set (ReLU and linear)\n"
      ],
      "metadata": {
        "id": "m8gRW8LIni_J"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e7pdJmTERyOe"
      },
      "outputs": [],
      "source": [
        "def linear_models(rep,data_set_name,scale = False,embedding_size = 2):\n",
        "\n",
        "    num_kernel = 2\n",
        "    num_gamma = len(gamma_list)\n",
        "    SVM_test_list = []\n",
        "    Knn_test_list = []\n",
        "    embed_test_list = np.load('embeddings/embed{}_{}_lin_test_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "    embed_lab_list = np.load('embeddings/embed{}_{}_lin_lab_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "    Y_test_list = np.load('embeddings/Y_test{}_{}_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "    Y_lab_list = np.load('embeddings/Y_lab{}_{}_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),allow_pickle=True)\n",
        "\n",
        "    idx = 0\n",
        "    for r in range(rep):\n",
        "        y_lab = Y_lab_list[r,:]\n",
        "        y_test = Y_test_list[r,:]\n",
        "        temp_SVM_test_list = []\n",
        "        temp_Knn_test_list = []\n",
        "\n",
        "        for i in range(4*2):\n",
        "\n",
        "            if scale:\n",
        "                scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(embed_lab_list[idx,:,:])\n",
        "                embed_test_list_ = scaling.transform(embed_test_list[idx,:,:])\n",
        "                embed_lab_list_ = scaling.transform(embed_lab_list[idx,:,:])\n",
        "            else:\n",
        "                embed_test_list_ = embed_test_list[idx,:,:]\n",
        "                embed_lab_list_ = embed_lab_list[idx,:,:]\n",
        "\n",
        "            ######### KNN\n",
        "            try:\n",
        "                neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "                neigh.fit(embed_lab_list_,y_lab)\n",
        "                neigh_predict = neigh.predict(embed_test_list_)\n",
        "                neigh_predict_accuracy = accuracy_score(y_test, neigh_predict)\n",
        "                temp_Knn_test_list.append(neigh_predict_accuracy)\n",
        "            except:\n",
        "                temp_Knn_test_list.append(0)\n",
        "\n",
        "            ######### SVM\n",
        "            try:\n",
        "                neigh = svm.SVC(kernel='linear')\n",
        "                neigh.fit(embed_lab_list_,y_lab)\n",
        "                neigh_predict = neigh.predict(embed_test_list_)\n",
        "                neigh_predict_accuracy = accuracy_score(y_test, neigh_predict)\n",
        "                temp_SVM_test_list.append(neigh_predict_accuracy)\n",
        "            except:\n",
        "                temp_Knn_test_list.append(0)\n",
        "\n",
        "\n",
        "            idx+=1\n",
        "        Knn_test_list.append(temp_Knn_test_list)\n",
        "        SVM_test_list.append(temp_SVM_test_list)\n",
        "\n",
        "    all_test_list_lin = np.array([Knn_test_list, SVM_test_list])\n",
        "    np.save('embeddings/all_test_list_lin{}_{}_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),all_test_list_lin)\n",
        "\n",
        "    # return SVM_test_list,Knn_test_list"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tNXBy3Kp3stk"
      },
      "source": [
        "# Benchmark on original features"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Knn and SVM on original data"
      ],
      "metadata": {
        "id": "798YWCgbnvRH"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "2kHWcmci3wE2"
      },
      "outputs": [],
      "source": [
        "def run_benchmark(X,Y,N_test,N_lab,rep,scale=False,embedding_size = 2):\n",
        "    all_benchmark_list = []\n",
        "\n",
        "    def ReLU_Kernel_PCA(x,y):\n",
        "        inp = (x@y.T)/gamma\n",
        "        np.clip(inp, -1, 1, out=inp)\n",
        "        K_ij = inp*kappa_0(inp) + kappa_1(inp)\n",
        "        return K_ij\n",
        "\n",
        "    for r in range(rep):\n",
        "\n",
        "        accuracy_list = []\n",
        "\n",
        "        # get data splits\n",
        "        X_train,X_lab,Y_lab,X_test,Y_test,X0,Xp,Xm = create_data_split(X,Y, N_lab,N_test,r,noise=0.1)\n",
        "\n",
        "        if scale:\n",
        "            scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X_lab)\n",
        "            X_lab = scaling.transform(X_lab)\n",
        "            X_test = scaling.transform(X_test)\n",
        "\n",
        "        #####################################\n",
        "\n",
        "        neigh = KNeighborsClassifier(n_neighbors=3)\n",
        "        neigh.fit(X_lab, Y_lab)\n",
        "        neigh_predict = neigh.predict(X_test)\n",
        "        neigh_predict_accuracy = accuracy_score(Y_test, neigh_predict)\n",
        "\n",
        "        accuracy_list.append(neigh_predict_accuracy)\n",
        "\n",
        "        #####################################\n",
        "\n",
        "        kernel_name_list = ['linear',ReLU_Kernel_PCA, 'rbf', sklearn.metrics.pairwise.laplacian_kernel]\n",
        "        gamma = ReLU_Kernel_normalize(X_lab)\n",
        "        for kernel in kernel_name_list:\n",
        "            try:\n",
        "                clf = svm.SVC(kernel=kernel)\n",
        "                clf.fit(X_lab,Y_lab)\n",
        "                SVM_predict = clf.predict(X_test)\n",
        "                SVM_predict_accuracy = accuracy_score(Y_test, SVM_predict)\n",
        "                accuracy_list.append(SVM_predict_accuracy)\n",
        "            except:\n",
        "                accuracy_list.append(0)\n",
        "\n",
        "        all_benchmark_list.append(accuracy_list)\n",
        "\n",
        "    np.save('embeddings/all_benchmark_list{}_{}_lin_test_list_{}.npy'.format(int(N_lab*100),embedding_size,data_set_name),all_benchmark_list)\n",
        "\n",
        "    # return all_benchmark_list"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "function that combines the main functions to run for the plots in Figure 1 as well as for the appendix"
      ],
      "metadata": {
        "id": "m7HBAEx-GbXQ"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9tgd1DodUYqu"
      },
      "outputs": [],
      "source": [
        "\n",
        "def run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name):\n",
        "    create_embedddings(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n",
        "    choose_best_gamma(rep,gamma_list,data_set_name)\n",
        "    linear_models(rep,data_set_name)\n",
        "    all_benchmark_list = run_benchmark(X,Y,N_test,N_lab,rep)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "P8x800cW6SyZ"
      },
      "source": [
        "# Run experiments\n",
        "\n",
        "Un-comment whatever dataset to run all experiments for"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WWD4sm-z6Y42"
      },
      "source": [
        "## Set General Parameters"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yyXzMvSt6Urb"
      },
      "outputs": [],
      "source": [
        "# data split\n",
        "N_unlab = 0.5\n",
        "N_lab = 0.05\n",
        "N_test = 0.45\n",
        "\n",
        "# hyperparameter list\n",
        "gamma_list = np.logspace(-2, 2, base = 10,num=25)\n",
        "\n",
        "# number of repetitions\n",
        "rep = 5"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Jmy53fe06ciy"
      },
      "source": [
        "## Circles"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "0rYK6ayWBhQZ"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.1\n",
        "# N_test = 0.4\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# n_samples = 200\n",
        "# factor = 0.6\n",
        "# data_set_name = 'circles_factor_{}'.format(int(factor*10))\n",
        "# n_clusters = 2\n",
        "\n",
        "# data = datasets.make_circles(n_samples=n_samples, factor=factor, noise=0.05, random_state=0)\n",
        "# X = data[0]\n",
        "# Y = data[1]\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n",
        "# plot_accuracy(data_set_name,N_lab)\n",
        "# run_NN(X,Y,N_test,N_lab,rep,scale=False,embedding_size = 2)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# rep = 5\n",
        "# acc_list = []\n",
        "# for cut in [0.01,0.5,1]:\n",
        "#     all_acc_list = create_embedddings_change_num_data(X,Y,N_lab,N_test,rep,data_set_name,cut,scale = False,embedding_size=2)\n",
        "#     print(all_acc_list)\n",
        "#     acc_list.append(all_acc_list)\n",
        "\n",
        "# np.save('embeddings/change_datanumber_{}.npy'.format(data_set_name),np.array(acc_list))\n"
      ],
      "metadata": {
        "id": "gT3WVeZbks-k"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "W6_XOXoaH4TL"
      },
      "source": [
        "\n",
        "## Half Moons"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.03\n",
        "# N_test = 0.47\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# n_samples = 200\n",
        "# data_set_name = 'moons'\n",
        "# n_clusters = 2\n",
        "\n",
        "# data = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
        "# X = data[0]\n",
        "# Y = data[1]\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ],
      "metadata": {
        "id": "HAptbjKrTOmU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b9Yv3BGP6xRO"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# n_samples = 200\n",
        "# data_set_name = 'moons'\n",
        "# n_clusters = 2\n",
        "\n",
        "# data = datasets.make_moons(n_samples=n_samples, noise=0.05)\n",
        "# X = data[0]\n",
        "# Y = data[1]\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vZiFPgVOIKeG"
      },
      "source": [
        "## Blobs"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZzfG8pye-jDU"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# n_samples = 200\n",
        "# data_set_name = 'blobs'\n",
        "# n_clusters = 3\n",
        "\n",
        "# data = datasets.make_blobs(n_samples=n_samples, random_state=8)\n",
        "# X = data[0]\n",
        "# Y = data[1]\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Cubes"
      ],
      "metadata": {
        "id": "vAo5CUdbhIND"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# n_samples = 200\n",
        "# data_set_name = 'cubes'\n",
        "# n_clusters = 2\n",
        "\n",
        "# data = sklearn.datasets.make_classification(n_samples=200,n_classes=n_clusters)\n",
        "# X = data[0]\n",
        "# Y = data[1]\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ],
      "metadata": {
        "id": "vQZizpOshKM3"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "uWfRY68SR-nq"
      },
      "source": [
        "## Iris"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "U7ONGdKRR_JJ"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# data_set_name = 'iris'\n",
        "# n_clusters = 3\n",
        "\n",
        "# from sklearn.datasets import load_iris\n",
        "# iris_data = load_iris()\n",
        "# X = iris_data.data\n",
        "# Y = iris_data.target\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0Nf5p9wYSL0m"
      },
      "source": [
        "## wine"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "b4DKT8SrSJjw"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "\n",
        "# data_set_name = 'wine'\n",
        "# n_clusters = 3\n",
        "\n",
        "# from sklearn.datasets import load_wine\n",
        "# wine_data = load_wine()\n",
        "# X = wine_data.data\n",
        "# Y = wine_data.target\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ypZXTTAeST48"
      },
      "source": [
        "## Brest cancer"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YjUn-ccgSWRe"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "\n",
        "# data_set_name = 'brest_cancer'\n",
        "# n_clusters = 2\n",
        "\n",
        "# from sklearn.datasets import load_breast_cancer\n",
        "# cancer_data = load_breast_cancer()\n",
        "# X = cancer_data.data\n",
        "# Y = cancer_data.target\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mJKAyW5LQyN5"
      },
      "source": [
        "## heart failure\n",
        "\n",
        "https://archive.ics.uci.edu/dataset/17/breast+cancer+wisconsin+diagnostic"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "kfLo0I3FOMC0"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "\n",
        "# data_set_name = 'heart_failure_scale'\n",
        "# n_clusters = 2\n",
        "\n",
        "# hear_data_import = np.genfromtxt('small_data_file/heart_failure_clinical_records_dataset.csv', delimiter=',')\n",
        "# X = hear_data_import[1:,:-1]\n",
        "# Y = hear_data_import[1:,-1]\n",
        "\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "e2WVm_yJFJ69"
      },
      "source": [
        "## Ionosphere"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "AAD1VuZbzLmy"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# data_set_name = 'ionosphere'\n",
        "# n_clusters = 2\n",
        "\n",
        "# # load dataset https://archive.ics.uci.edu/ml/datasets/ionosphere\n",
        "# data = pd.read_csv(\"small_data_file/ionosphere.csv\", header=None)\n",
        "# col_names = list(data.columns)\n",
        "# feature_names = list(col_names[:-1])\n",
        "# target_names = ['bad', 'good']\n",
        "\n",
        "# cols = list(data)\n",
        "# cols.insert(0, cols.pop(cols.index(34)))\n",
        "# data = data.loc[:, cols]\n",
        "# data.columns = col_names\n",
        "\n",
        "# X = np.float32(data.drop(0, axis=1).values)\n",
        "# Y = data[0].values\n",
        "\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "0NoPYaI8G80K"
      },
      "source": [
        "## Mushroom"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "HNft3GsU4jQi"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "\n",
        "\n",
        "# data_set_name = 'Mushroom_sample'\n",
        "# n_clusters = 2\n",
        "\n",
        "# #https://github.com/thomberg1/SmallDatasetsAnalysis/blob/master/mushroom_ML_classifier.ipynb\n",
        "\n",
        "# dataframe = pd.read_csv(\"small_data_file/agaricus-lepiota.csv\", header=None)\n",
        "\n",
        "# target_names = ['edible', 'poisonous']\n",
        "\n",
        "# #Transform data form categorial to numeric\n",
        "# cat_variables = [i for i in dataframe.columns if dataframe[i].dtype == 'object']\n",
        "# encods = [LabelEncoder() for col in cat_variables]\n",
        "# for i, col in enumerate(cat_variables):\n",
        "#     dataframe[col] = encods[i].fit_transform(dataframe[col].astype(str))\n",
        "\n",
        "# n = 200\n",
        "# X = np.float32(dataframe.values[:,1:])\n",
        "# print(X.shape)\n",
        "# Y = np.int64(dataframe.values[:,0])\n",
        "# X,Y = shuffle(X,Y,random_state=0)\n",
        "\n",
        "# Y = Y[:n]\n",
        "# X = X[:n,:]\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MLfqc1kvHg_L"
      },
      "source": [
        "## MNIST"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "iR-pWlEU6UVr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "99c8f7b3-2995-464d-83be-91ac8261a12c"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "MNIST loaded dataset Dataset MNIST\n",
            "    Number of datapoints: 10000\n",
            "    Root location: ./data\n",
            "    Split: Test\n",
            "Number of total samples  torch.Size([500])\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:80: UserWarning: test_data has been renamed data\n",
            "  warnings.warn(\"test_data has been renamed data\")\n",
            "<ipython-input-64-3a696988cf73>:22: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X_full = torch.tensor(data.test_data.reshape(data.test_data.shape[0],-1))\n",
            "/usr/local/lib/python3.10/dist-packages/torchvision/datasets/mnist.py:70: UserWarning: test_labels has been renamed targets\n",
            "  warnings.warn(\"test_labels has been renamed targets\")\n",
            "<ipython-input-64-3a696988cf73>:23: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  Y_full = torch.tensor(data.test_labels)\n",
            "<ipython-input-64-3a696988cf73>:33: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
            "  X = torch.tensor(X_full[x_idx], dtype=torch.long).detach().cpu().numpy()\n"
          ]
        }
      ],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "# rep = 5\n",
        "\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from collections import defaultdict\n",
        "# import torchvision\n",
        "\n",
        "# data_set_name = 'mnist_sample_2'\n",
        "# n_clusters = 2\n",
        "\n",
        "# class_labels_list = [0,1]\n",
        "# num_samples = 250\n",
        "\n",
        "# data = torchvision.datasets.MNIST(root='./data', train=False, download=True, transform=None)\n",
        "# print('MNIST loaded dataset', data)\n",
        "\n",
        "# X_full = torch.tensor(data.test_data.reshape(data.test_data.shape[0],-1))\n",
        "# Y_full = torch.tensor(data.test_labels)\n",
        "\n",
        "# #pick classes in class_labels_list, num_samples from each\n",
        "# x_idx = torch.tensor([])\n",
        "# for i in class_labels_list:\n",
        "#     x_idx = torch.cat((x_idx,(Y_full==i).nonzero(as_tuple=True)[0][:num_samples]))\n",
        "# print('Number of total samples ', x_idx.shape)\n",
        "\n",
        "# shuffle_idx = torch.randperm(x_idx.shape[0])\n",
        "# x_idx = x_idx[shuffle_idx].long()\n",
        "# X = torch.tensor(X_full[x_idx], dtype=torch.long).detach().cpu().numpy()\n",
        "# Y = Y_full[x_idx].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n",
        "# # run_NN(X,Y,N_test,N_lab,rep,scale=False,embedding_size = 2)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## CIFAR"
      ],
      "metadata": {
        "id": "YjKNlu2x909Y"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "5hKa4bzrKO0d"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.3\n",
        "# N_test = 0.2\n",
        "\n",
        "# rep = 2\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from collections import defaultdict\n",
        "# import torchvision\n",
        "\n",
        "# data_set_name = 'cifar_sample_2'\n",
        "# n_clusters = 2\n",
        "\n",
        "# class_labels_list = [0,1]\n",
        "# num_samples = 250\n",
        "\n",
        "# data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
        "# print('CIFAR10 loaded dataset', data)\n",
        "\n",
        "# X_full = torch.tensor(data.data.reshape(data.data.shape[0],-1))\n",
        "# Y_full = torch.tensor(data.targets)\n",
        "\n",
        "# #pick classes in class_labels_list, num_samples from each\n",
        "# x_idx = torch.tensor([])\n",
        "# for i in class_labels_list:\n",
        "#     x_idx = torch.cat((x_idx,(Y_full==i).nonzero(as_tuple=True)[0][:num_samples]))\n",
        "# print('Number of total samples ', x_idx.shape)\n",
        "\n",
        "# shuffle_idx = torch.randperm(x_idx.shape[0])\n",
        "# x_idx = x_idx[shuffle_idx].long()\n",
        "# X = torch.tensor(X_full[x_idx], dtype=torch.long).detach().cpu().numpy()\n",
        "# Y = Y_full[x_idx].detach().cpu().numpy()\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n",
        "\n",
        "# create_embedddings(X,Y,N_lab,N_test,rep,gamma_list,data_set_name,scale = True,embedding_size= 10)\n",
        "# choose_best_gamma(rep,gamma_list,data_set_name,scale = True)\n",
        "# linear_models(rep,data_set_name,scale = True)\n",
        "# all_benchmark_list = run_benchmark(X,Y,N_test,N_lab,rep)\n",
        "\n",
        "# run_NN(X,Y,N_test,N_lab,rep,scale=True,embedding_size = 10)\n"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from collections import defaultdict\n",
        "# import torchvision\n",
        "\n",
        "# data_set_name = 'cifar_sample_5'\n",
        "# data_set_name = 'SVHN_sample_5'\n",
        "# n_clusters = 5\n",
        "\n",
        "# class_labels_list = [0,1,2,3,4,5]\n",
        "# num_samples = 50\n",
        "\n",
        "# data = torchvision.datasets.CIFAR10(root='./data', train=False, download=True, transform=None)\n",
        "# print('CIFAR10 loaded dataset', data)\n",
        "\n",
        "# X_full = torch.tensor(data.data.reshape(data.data.shape[0],-1))\n",
        "# Y_full = torch.tensor(data.targets)\n",
        "\n",
        "# #pick classes in class_labels_list, num_samples from each\n",
        "# x_idx = torch.tensor([])\n",
        "# for i in class_labels_list:\n",
        "#     x_idx = torch.cat((x_idx,(Y_full==i).nonzero(as_tuple=True)[0][:num_samples]))\n",
        "# print('Number of total samples ', x_idx.shape)\n",
        "\n",
        "# shuffle_idx = torch.randperm(x_idx.shape[0])\n",
        "# x_idx = x_idx[shuffle_idx].long()\n",
        "# X = torch.tensor(X_full[x_idx], dtype=torch.long).detach().cpu().numpy()\n",
        "# Y = Y_full[x_idx].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "# _ = run_de_noising_experiment(X,Y,noise = 0.1,N_lab=N_lab,N_test=N_test,rep=5,gamma_list=gamma_list,data_set_name=data_set_name,epochs = 500)"
      ],
      "metadata": {
        "id": "tkK8BMpXWUAu"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## SVHN"
      ],
      "metadata": {
        "id": "WBxLp2dkDzis"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from collections import defaultdict\n",
        "# import torchvision\n",
        "\n",
        "# data_set_name = 'SVHN_sample_5'\n",
        "# n_clusters = 5\n",
        "\n",
        "# class_labels_list = [0,1,2,3,4,5]\n",
        "# num_samples = 50\n",
        "\n",
        "# data = torchvision.datasets.SVHN(root='./data',download=True)\n",
        "# print('SVHN loaded dataset', data)\n",
        "\n",
        "# X_full = torch.tensor(data.data.reshape(data.data.shape[0],-1))\n",
        "# Y_full = torch.tensor(data.labels)\n",
        "\n",
        "# #pick classes in class_labels_list, num_samples from each\n",
        "# x_idx = torch.tensor([])\n",
        "# for i in class_labels_list:\n",
        "#     x_idx = torch.cat((x_idx,(Y_full==i).nonzero(as_tuple=True)[0][:num_samples]))\n",
        "# print('Number of total samples ', x_idx.shape)\n",
        "\n",
        "# shuffle_idx = torch.randperm(x_idx.shape[0])\n",
        "# x_idx = x_idx[shuffle_idx].long()\n",
        "# X = torch.tensor(X_full[x_idx], dtype=torch.long).detach().cpu().numpy()\n",
        "# Y = Y_full[x_idx].detach().cpu().numpy()\n",
        "\n",
        "# all_mse_list = run_de_noising_experiment(X,Y,noise = 0.1,N_lab=N_lab,N_test=N_test,rep=5,gamma_list=gamma_list,data_set_name=data_set_name,epochs = 500)"
      ],
      "metadata": {
        "id": "SFfziEljDz_D"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.05\n",
        "# N_test = 0.45\n",
        "\n",
        "\n",
        "# ### Get data\n",
        "# import pandas as pd\n",
        "# from sklearn.preprocessing import LabelEncoder\n",
        "# from collections import defaultdict\n",
        "# import torchvision\n",
        "\n",
        "# data_set_name = 'SVHN_sample_2'\n",
        "# n_clusters = 2\n",
        "\n",
        "# class_labels_list = [0,1]\n",
        "# num_samples = 250\n",
        "\n",
        "# data = torchvision.datasets.SVHN(root='./data',download=True)\n",
        "# print('SVHN loaded dataset', data)\n",
        "\n",
        "# X_full = torch.tensor(data.data.reshape(data.data.shape[0],-1))\n",
        "# Y_full = torch.tensor(data.labels)\n",
        "\n",
        "# #pick classes in class_labels_list, num_samples from each\n",
        "# x_idx = torch.tensor([])\n",
        "# for i in class_labels_list:\n",
        "#     x_idx = torch.cat((x_idx,(Y_full==i).nonzero(as_tuple=True)[0][:num_samples]))\n",
        "# print('Number of total samples ', x_idx.shape)\n",
        "\n",
        "# shuffle_idx = torch.randperm(x_idx.shape[0])\n",
        "# x_idx = x_idx[shuffle_idx].long()\n",
        "# X = torch.tensor(X_full[x_idx], dtype=torch.long).detach().cpu().numpy()\n",
        "# Y = Y_full[x_idx].detach().cpu().numpy()\n",
        "\n",
        "\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n",
        "\n",
        "# # create_embedddings(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n",
        "# choose_best_gamma(rep,gamma_list,data_set_name,scale = True)\n",
        "# linear_models(rep,data_set_name)\n",
        "# all_benchmark_list = run_benchmark(X,Y,N_test,N_lab,rep)\n",
        "\n",
        "# run_NN(X,Y,N_test,N_lab,rep,scale=True,embedding_size = 2)"
      ],
      "metadata": {
        "id": "d7P1K7-IUfhI"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cICMVlD0n_xZ"
      },
      "source": [
        "## Parkinsons Disease Classification\n",
        "\n",
        "https://github.com/imadtoubal/Parkinson-s-Disease-Classification-from-Speech-Data"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "i5EVbN3BOT_3"
      },
      "outputs": [],
      "source": [
        "# N_unlab = 0.5\n",
        "# N_lab = 0.1\n",
        "# N_test = 0.4\n",
        "\n",
        "# # ### Get data\n",
        "\n",
        "\n",
        "# data_set_name = 'pd_speech'\n",
        "# n_clusters = 2\n",
        "\n",
        "# df = pd.read_csv('small_data_file/pd_speech_features.txt')\n",
        "# df.drop(['id'], 1, inplace=True)\n",
        "# X = np.array(df.drop(['class'], 1))\n",
        "\n",
        "# print(X.shape)\n",
        "# Y = np.array(df['class'])\n",
        "\n",
        "# scaling = sklearn.preprocessing.MinMaxScaler(feature_range=(0,1)).fit(X)\n",
        "# X = scaling.transform(X)\n",
        "\n",
        "# # run_experiments(X,Y,N_lab,N_test,rep,gamma_list,data_set_name)\n"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "toc_visible": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}